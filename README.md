# ComputaÃ§Ã£o Paralela â€” LaboratÃ³rios

Este repositÃ³rio contÃ©m os **laboratÃ³rios desenvolvidos na disciplina de ComputaÃ§Ã£o Paralela**, com foco em **programaÃ§Ã£o paralela, anÃ¡lise de desempenho e otimizaÃ§Ãµes de memÃ³ria**, utilizando **OpenMP**, ferramentas de profiling e conceitos clÃ¡ssicos da Ã¡rea.

Os trabalhos exploram desde paralelismo em CPU atÃ© noÃ§Ãµes introdutÃ³rias de aceleraÃ§Ã£o em GPU.

---

## ğŸ“Œ ConteÃºdos Abordados

- Paralelismo de dados e de tarefas
- Modelo de execuÃ§Ã£o **Fork-Join**
- **OpenMP**: `parallel`, `for`, `schedule`, `reduction`, `critical`, `atomic`, `simd`, `task`
- AnÃ¡lise de **localidade de memÃ³ria** (espacial e temporal)
- **Cache misses**, cache thrashing e false sharing
- **Blocking (matriz blocada)**
- Leis de **Amdahl** e **Gustafson**
- Profiling de desempenho com **Valgrind / Cachegrind** e **perf**
- IntroduÃ§Ã£o a **OpenACC** e modelo **Hostâ€“Device (CPUâ€“GPU)**

---

## ğŸ§ª Estrutura dos LaboratÃ³rios

### ğŸ”¹ Lab 01 â€“ IntroduÃ§Ã£o ao Paralelismo
- Conceitos bÃ¡sicos de paralelismo
- Modelo Fork-Join
- Primeiros exemplos com OpenMP

---

### ğŸ”¹ Lab 02 â€“ Paralelismo de LaÃ§os (`omp for`)
- ParalelizaÃ§Ã£o de laÃ§os `for`
- Balanceamento de carga
- Uso das polÃ­ticas `schedule(static, dynamic, guided)`

---

### ğŸ”¹ Lab 03 â€“ CondiÃ§Ã£o de Corrida e SincronizaÃ§Ã£o
- IdentificaÃ§Ã£o de **race conditions**
- Uso de:
  - `critical`
  - `atomic`
  - `reduction`
- ComparaÃ§Ã£o de desempenho entre abordagens

---

### ğŸ”¹ Lab 04 â€“ Projeto de Algoritmos Paralelos I
- MultiplicaÃ§Ã£o de matrizes
- ComparaÃ§Ã£o entre ordens de laÃ§o (**IJK vs IKJ**)
- Impacto da **localidade de cache**
- Mapeamento 1D e problemas de acesso Ã  memÃ³ria

---

### ğŸ”¹ Lab 05 â€“ Projeto de Algoritmos Paralelos II
- Paralelismo de tarefas com `omp task`
- ImplementaÃ§Ã£o paralela do **Quicksort**
- DiferenÃ§a entre:
  - Paralelismo de dados
  - Fila de tarefas

---

### ğŸ”¹ Lab 06 â€“ Projeto de Algoritmos Paralelos III
- Algoritmos iterativos com mÃºltiplas fases
- **Odd-Even Sort paralelo**
- AnÃ¡lise de:
  - Barreiras implÃ­citas
  - Overhead de sincronizaÃ§Ã£o
  - Escalabilidade limitada

---

### ğŸ”¹ Lab 07 â€“ Localidade e OtimizaÃ§Ã£o de Cache
- **Blocking (matriz blocada)**
- Localidade espacial vs temporal
- ReduÃ§Ã£o de cache misses
- ComparaÃ§Ã£o entre versÃµes serial e paralela

---

### ğŸ”¹ Lab 08 â€“ Profiling e AnÃ¡lise de Desempenho
- Uso do **Valgrind / Cachegrind**
- MÃ©tricas analisadas:
  - Cache hits e misses
  - Taxa de miss
  - InstruÃ§Ãµes executadas
- IdentificaÃ§Ã£o de gargalos de memÃ³ria

---

### ğŸ”¹ Lab 09 â€“ Estudo de Caso Completo
- Processamento massivo de dados
- ConstruÃ§Ã£o e uso de **Tabela Hash**
- ParalelizaÃ§Ã£o com OpenMP
- AnÃ¡lise de:
  - Parede da MemÃ³ria
  - Parede da SincronizaÃ§Ã£o

---

### ğŸ”¹ Lab 10 â€“ IntroduÃ§Ã£o Ã  ProgramaÃ§Ã£o em GPU
- Modelo **Hostâ€“Device**
- IntroduÃ§Ã£o ao **OpenACC**
- TransferÃªncia de dados via PCIe
- Conceito da **Muralha do PCIe**

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **C**
- **OpenMP**
- **Valgrind / Cachegrind**
- **perf**
- **OpenACC**
- GCC / Clang
- Linux / WSL

---

## â–¶ï¸ CompilaÃ§Ã£o

Exemplo de compilaÃ§Ã£o com OpenMP:
```bash
gcc -fopenmp -O2 arquivo.c -o programa